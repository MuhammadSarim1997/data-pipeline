# Airflow + Docker + DBT + Airbyte Data Pipeline

This repository contains a modular, end-to-end ELT pipeline that leverages **Airflow**, **Docker**, **DBT**, and **Airbyte** to orchestrate, extract, load, and transform data across Postgres databases.

## ğŸ”§ Tech Stack

- **Apache Airflow**: Workflow orchestration
- **Airbyte**: Data ingestion and syncing between databases
- **Docker Compose**: Containerization and environment management
- **PostgreSQL**: Source and destination databases
- **DBT**: Data transformation and modeling
- **Python**: DAG logic and Airbyte API integration

## ğŸ“ Project Structure

<pre>
data-pipeline/
â”œâ”€â”€ airflow/                  # Airflow config & DAGs
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ Airbyte_conection.py
â”‚   â”‚   â””â”€â”€ elt_dag.py
â”‚   â””â”€â”€ airflow.cfg
â”œâ”€â”€ custom_postgres/          # DBT transformation project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ macros/
â”œâ”€â”€ Dockerfile                # Custom Airflow image with providers
â”œâ”€â”€ docker-compose.yml        # Service orchestration
â”œâ”€â”€ source_db_init/           # SQL seed script for source Postgres
â”œâ”€â”€ .env                      # Environment variables
â””â”€â”€ .gitignore
</pre>

## âœ… Prerequisites
Make sure the following tools are installed before you run the pipeline:
  - Docker â€“ Required for containerizing all services
  - Docker Compose â€“ For orchestrating multi-container environments
  - abctl (Airbyte CLI) â€“ Used to install and manage Airbyte via Kubernetes-in-Docker (automatically handled via script)
  - Python 3.8+ â€“ Required to run custom Python DAG logic (e.g., API calls to Airbyte)

ğŸ’¡ Tip: No need to manually install Airbyte â€“ the elt.sh script will install it for you using install_abctl.sh.

## âš™ï¸ Setup Instructions

### 1. Clone the Repository


'''bash
git clone https://github.com/MuhammadSarim1997/data-pipeline.git
cd data-pipeline
bash'''
  
### 2. Add Environment Variables
Create a .env file :

<pre>
AB_client_id="your_airbyte_client_id"
AB_client_secret="your_airbyte_client_secret"
AB_connection_id="your_airbyte_connection_id"
AB_workspace_id="your_airbyte_workspace_id"

POSTGRES_USER=postgres
POSTGRES_PASSWORD=secret
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflow
AIRFLOW_USERNAME=airflow
AIRFLOW_PASSWORD=password
AIRFLOW_FERNET_KEY=your_fernet_key
</pre>
  
### 3. Build and Start the Pipeline
<pre>
bash ./elt.sh
</pre>
This script:
  - Installs Airbyte via abctl if needed
  - Loads the environment
  - Initializes Airflow
  - Starts Airflow, Airbyte, and Postgres services via Docker

### 4. Access Interfaces
Service	URL
<pre>
Airflow UI	http://localhost:8080
Airbyte UI	http://localhost:8000
Source DB	localhost:5433
Destination DB	localhost:5434
</pre>
## âœ… Features
  - Automated Airbyte syncs via Airflow DAG
  - Polling and job status tracking with Airbyte API
  - Custom transformations using dbt
  - Containerized setup for easy deployment
  
## ğŸ“Œ TODO
- Add logging and error alerts to DAG
- Extend to S3 or Snowflake
- Set up GitHub Actions for CI/CD

## ğŸ“œ License
MIT License â€” feel free to fork and extend.
